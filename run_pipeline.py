"""
run_pipeline.py
- Robust and modular end-to-end trading pipeline
- Workflow: Download data -> Generate features -> Train models (optional)
- Execute backtest -> Execute benchmark.py -> Combine results for dashboard
"""

import subprocess
import argparse
import os
import pandas as pd
import sys

def run_cmd(cmd, ignore_error=False):
    """Executes a shell command and prints output"""
    print(f"\n➡️ RUN: {cmd}")
    rc = subprocess.run(cmd, shell=True)
    if rc.returncode != 0:
        msg = f"❌ Command failed: {cmd}"
        if ignore_error:
            print(msg + " (Ignored)")
        else:
            raise RuntimeError(msg)

def models_exist():
    """Check if trained models already exist"""
    required = ["models/decision_tree.joblib",
                "models/random_forest.joblib",
                "models/xgboost.joblib",
                "models/best_model.joblib"]
    return all(os.path.exists(f) for f in required)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--start", default="2010-01-01")
    parser.add_argument("--end", default=None)
    parser.add_argument("--tickers_source", default="sp500")
    parser.add_argument("--train", action="store_true", help="Force training of new models")
    parser.add_argument("--benchmark_symbol", default="SPY", help="Symbol for the benchmark")
    args = parser.parse_args()

    # 1. Download data
    end_arg = f"--end {args.end}" if args.end else ""
    run_cmd(f"python scripts/data_fetch.py --start {args.start} {end_arg} --source {args.tickers_source}")

    # 2. Generate features
    run_cmd(f"{sys.executable} scripts/features.py")

    # 3. Train models if they don't exist or training is forced
    if args.train or not models_exist():
        print("➡️ Training new models...")
        run_cmd("python scripts/train_models.py")
    else:
        print("➡️ Using existing models in models/")

    # 4. Run backtest
    run_cmd("python scripts/backtest.py")

    # 5. Execute benchmark.py
    run_cmd(f"python scripts/benchmark.py --start {args.start} {end_arg} --symbol {args.benchmark_symbol}")

    # 6. Load backtest results
    backtest_csv = "output/portfolio_threshold_series.csv"
    metrics_csv = "output/backtest_metrics.csv"
    if os.path.exists(backtest_csv):
        portfolio = pd.read_csv(backtest_csv, index_col=0, parse_dates=True)

        # Ensure Strategy_Cumulative exists and is normalized
        if "Strategy_Cumulative" not in portfolio.columns:
            portfolio["Strategy_Cumulative"] = portfolio["Portfolio"] / portfolio["Portfolio"].iloc[0]
        else:
            portfolio["Strategy_Cumulative"] = portfolio["Strategy_Cumulative"] / portfolio["Strategy_Cumulative"].iloc[0]

        print("✅ Portfolio loaded and Strategy_Cumulative normalized")
    else:
        raise FileNotFoundError("output/portfolio_threshold_series.csv not found")

    if os.path.exists(metrics_csv):
        metrics = pd.read_csv(metrics_csv)
    else:
        metrics = pd.DataFrame()

    # 7. Load benchmark generated by benchmark.py
    benchmark_csv = f"output/{args.benchmark_symbol}_benchmark.csv"
    if os.path.exists(benchmark_csv):
        benchmark = pd.read_csv(benchmark_csv, index_col=0, parse_dates=True)
        benchmark.rename(columns={benchmark.columns[0]: f"{args.benchmark_symbol}_Cumulative"}, inplace=True)
        print(f"✅ Benchmark {args.benchmark_symbol} loaded from benchmark.py")
    else:
        benchmark = pd.DataFrame()
        print("⚠️ Benchmark CSV not found; skipping")

    # 8. Combine curves for dashboard
    combined = portfolio.copy()
    if not benchmark.empty:
        combined = combined.join(benchmark, how="left")
    combined.to_csv("output/dashboard_data.csv")
    print("✅ Combined curves saved to output/dashboard_data.csv")

    # 9. Save combined metrics
    if not metrics.empty and not benchmark.empty:
        b_cum = benchmark.iloc[-1, 0] / benchmark.iloc[0, 0]
        years = len(benchmark) / 252
        metrics["Benchmark_CAGR"] = b_cum ** (1/years) - 1
        metrics.to_csv("output/dashboard_metrics.csv", index=False)
        print("✅ Combined metrics saved to output/dashboard_metrics.csv")

    print("\n✅ Pipeline finished. Check output/, models/, and generated plots.")








